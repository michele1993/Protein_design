{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNstfntMMP+BNudjgkf2Cl7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michele1993/Protein_design/blob/main/PropGPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4MkLTsyLmE4R"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('content')\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir dataset"
      ],
      "metadata": {
        "id": "brxgXA_dyszT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data clearning\n",
        "Need to clean the data and split them in training and validation dataset for fine-tuning PropGPT2"
      ],
      "metadata": {
        "id": "33Y8k7S2oq9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "#root_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "file_path = os.path.join('dataset','sequences.csv')\n",
        "\n",
        "dataset = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "9nOsO_hypoC5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ------ 1st Remove any pair if contains NaN -----\n",
        "# identify any row with NaN\n",
        "data_nan = pd.isna(dataset).sum(axis=1).astype('bool') # sum to identify if there is at least one NaN entry in a row.\n",
        "# remove entries with NaN entirely\n",
        "data_cleaned = dataset.loc[~data_nan, :]\n",
        "\n",
        "assert ~pd.isna(data_cleaned).any().any(), \"There are NaN entries in the data, need cleaning\"\n",
        "## ----------------------------"
      ],
      "metadata": {
        "id": "EwuXwIkpqEnk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ------ 2nd Remove any duplicate entry ----\n",
        "\n",
        "# try adding a duplicate to test if works:\n",
        "#data_cleaned = pd.concat([data_cleaned, pd.DataFrame(data_cleaned.iloc[-1,:], columns=data_cleaned.columns)], ignore_index=True)\n",
        "#data_cleaned.loc[len(data_cleaned.index)] = data_cleaned.iloc[0,:]\n",
        "#print(data_cleaned.shape)\n",
        "\n",
        "# find all duplicates:\n",
        "#duplicates = data_cleaned[data_cleaned.duplicated(subset=\"mutated_sequence\", keep=False)]\n",
        "\n",
        "# Remove duplicates by only keeping 'first' occurance for each\n",
        "data_cleaned = data_cleaned.drop_duplicates(subset=\"mutated_sequence\", keep=\"first\")\n",
        "#print(data_cleaned.shape)\n",
        "## -------------------------------------------"
      ],
      "metadata": {
        "id": "44L5eS6MqFCh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ------- 3rd Prepara data for fine-tuning PropGPT2 on this dataset -----\n",
        "# for the moment ingore the activations just with all sequences\n",
        "data_cleaned_seq = data_cleaned.iloc[:,0]\n",
        "\n",
        "# 1st need to add \"<|endoftext|>\" token at the beginning of each seq\n",
        "special_token = \"<|endoftext|>\"\n",
        "data_cleaned_seq = special_token + data_cleaned_seq\n",
        "\n",
        "# 2nd need to slip the data in training and validation\n",
        "\n",
        "# Select % of validation seqs (i.e., 90/10)\n",
        "n_seq = data_cleaned_seq.shape[0]\n",
        "n_validation = n_seq // 10\n",
        "\n",
        "# Select n. random indexes for validation\n",
        "val_indx = np.random.randint(0, n_seq, n_validation)\n",
        "# Select validation seqs\n",
        "val_seq = data_cleaned_seq.iloc[val_indx]\n",
        "# Select training seqs by eliminating validation seqs\n",
        "training_seq = data_cleaned_seq.drop(index=val_indx)\n",
        "\n",
        "# 3rd: concatane all strings together and save in a txt file\n",
        "training_concatenated = ''.join(training_seq)\n",
        "val_concatenated = ''.join(val_seq)\n",
        "\n",
        "# to add newline character between each original row's string use\n",
        "#concatenated = '\\n'.join(training_seq)\n",
        "\n",
        "# Save concatenated strings\n",
        "with open('training.txt','w') as file:\n",
        "    file.write(training_concatenated)\n",
        "\n",
        "with open('validation.txt','w') as file:\n",
        "    file.write(val_concatenated)"
      ],
      "metadata": {
        "id": "upCliY28qSjP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, we can try to fine-tune PropGPT2 based on our data\n",
        "\n",
        "First we need to install some dependecies"
      ],
      "metadata": {
        "id": "t2dYmPQzrZF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! wget https://raw.githubusercontent.com/huggingface/transformers/refs/heads/main/examples/pytorch/language-modeling/run_clm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_4IvvQgxzKV",
        "outputId": "9d031eff-923e-4c18-e857-d73380994959"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-18 18:02:39--  https://raw.githubusercontent.com/huggingface/transformers/refs/heads/main/examples/pytorch/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28424 (28K) [text/plain]\n",
            "Saving to: ‘run_clm.py’\n",
            "\n",
            "\rrun_clm.py            0%[                    ]       0  --.-KB/s               \rrun_clm.py          100%[===================>]  27.76K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-10-18 18:02:39 (20.7 MB/s) - ‘run_clm.py’ saved [28424/28424]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cflBxAwVr7DK",
        "outputId": "11a82abe-2cb1-4221-eee4-23752745c87a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 235896, done.\u001b[K\n",
            "remote: Counting objects: 100% (21606/21606), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1527/1527), done.\u001b[K\n",
            "remote: Total 235896 (delta 21130), reused 20112 (delta 20054), pack-reused 214290 (from 1)\u001b[K\n",
            "Receiving objects: 100% (235896/235896), 241.61 MiB | 13.26 MiB/s, done.\n",
            "Resolving deltas: 100% (172884/172884), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -e transformers/."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B4IRnU9TsMRV",
        "outputId": "3cf38c6b-93fe-41b3-d8ca-7d688239e6ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (2.32.3)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.0.dev0)\n",
            "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (2024.8.30)\n",
            "Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.46.0.dev0-0.editable-py3-none-any.whl size=17265 sha256=c61d223aa9fdb3e0f32bbf848f6a9a5cbb4c53349b18bdb6f35e9c8f09880b7e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e984sj5u/wheels/7c/35/80/e946b22a081210c6642e607ed65b2a5b9a4d9259695ee2caf5\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed tokenizers-0.20.1 transformers-4.46.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "upgtKNs6q-0K",
        "outputId": "b56f4d99-6977-4b50-e1c2-d406a34efb4a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 evaluate-0.4.3 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python run_clm.py --model_name_or_path nferruz/ProtGPT2 --train_file training.txt --validation_file validation.txt --tokenizer_name nferruz/ProtGPT2 --do_train --do_eval --output_dir output --learning_rate 1e-06"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9pzHAM5q13Z",
        "outputId": "1f3a2c19-a033-4f75-cb87-eeaa9683836b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-18 18:03:54.727157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-18 18:03:54.746677: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-18 18:03:54.752530: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-18 18:03:54.766879: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-18 18:03:55.926808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/18/2024 18:04:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/18/2024 18:04:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-06,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/runs/Oct18_18-04-01_544273686d9b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=output,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-8231a25b6c0423f6\n",
            "10/18/2024 18:04:01 - INFO - datasets.builder - Using custom data configuration default-8231a25b6c0423f6\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n",
            "10/18/2024 18:04:01 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n",
            "Generating dataset text (/root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99)\n",
            "10/18/2024 18:04:01 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99...\n",
            "10/18/2024 18:04:01 - INFO - datasets.builder - Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99...\n",
            "Downloading took 0.0 min\n",
            "10/18/2024 18:04:01 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "10/18/2024 18:04:01 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "10/18/2024 18:04:01 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 1 examples [00:00, 31.54 examples/s]\n",
            "Generating validation split\n",
            "10/18/2024 18:04:01 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 1 examples [00:00, 1001.03 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "10/18/2024 18:04:01 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99. Subsequent calls will reuse this data.\n",
            "10/18/2024 18:04:01 - INFO - datasets.builder - Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99. Subsequent calls will reuse this data.\n",
            "config.json: 100% 850/850 [00:00<00:00, 4.95MB/s]\n",
            "[INFO|configuration_utils.py:678] 2024-10-18 18:04:01,996 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/config.json\n",
            "[INFO|configuration_utils.py:745] 2024-10-18 18:04:01,997 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"nferruz/ProtGPT2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1280,\n",
            "  \"n_head\": 20,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 36,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:706] 2024-10-18 18:04:02,083 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:678] 2024-10-18 18:04:02,174 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/config.json\n",
            "[INFO|configuration_utils.py:745] 2024-10-18 18:04:02,175 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"nferruz/ProtGPT2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1280,\n",
            "  \"n_head\": 20,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 36,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "vocab.json: 100% 655k/655k [00:00<00:00, 18.2MB/s]\n",
            "merges.txt: 100% 314k/314k [00:00<00:00, 2.42MB/s]\n",
            "tokenizer.json: 100% 1.07M/1.07M [00:00<00:00, 4.08MB/s]\n",
            "special_tokens_map.json: 100% 357/357 [00:00<00:00, 2.60MB/s]\n",
            "[INFO|tokenization_utils_base.py:2208] 2024-10-18 18:04:03,728 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2208] 2024-10-18 18:04:03,728 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2208] 2024-10-18 18:04:03,728 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2208] 2024-10-18 18:04:03,729 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2208] 2024-10-18 18:04:03,729 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2208] 2024-10-18 18:04:03,729 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:678] 2024-10-18 18:04:03,729 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/config.json\n",
            "[INFO|configuration_utils.py:745] 2024-10-18 18:04:03,730 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"nferruz/ProtGPT2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1280,\n",
            "  \"n_head\": 20,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 36,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.46.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "pytorch_model.bin: 100% 3.13G/3.13G [00:28<00:00, 110MB/s]\n",
            "[INFO|modeling_utils.py:3905] 2024-10-18 18:04:32,915 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--nferruz--ProtGPT2/snapshots/44255568d9f72bbfa05b23d3826599327ca37910/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:1099] 2024-10-18 18:04:33,024 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|safetensors_conversion.py:61] 2024-10-18 18:04:33,071 >> Attempting to create safetensors variant\n",
            "[INFO|modeling_utils.py:4767] 2024-10-18 18:04:33,399 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4775] 2024-10-18 18:04:33,399 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at nferruz/ProtGPT2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|safetensors_conversion.py:24] 2024-10-18 18:04:33,468 >> Attempting to convert .bin model on the fly to safetensors.\n",
            "[INFO|modeling_utils.py:4239] 2024-10-18 18:04:33,506 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99/cache-cf926d867279855a.arrow\n",
            "10/18/2024 18:04:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99/cache-cf926d867279855a.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 13.82 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99/cache-94cfef60e80cb176.arrow\n",
            "10/18/2024 18:04:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99/cache-94cfef60e80cb176.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 127.12 examples/s]\n",
            "10/18/2024 18:04:33 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Using block_size=1024 instead. You can change that default value by passing --block_size xxx.\n",
            "Grouping texts in chunks of 1024:   0% 0/1 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99/cache-f6d4cef45105b59e.arrow\n",
            "10/18/2024 18:04:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99/cache-f6d4cef45105b59e.arrow\n",
            "Grouping texts in chunks of 1024: 100% 1/1 [00:00<00:00, 40.90 examples/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/1 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99/cache-71ba9b500ff3c0b1.arrow\n",
            "10/18/2024 18:04:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-8231a25b6c0423f6/0.0.0/37eaf37ac90527a7fd768c94b312ee84f8815c9b7ac00acf81c1c364e8392f99/cache-71ba9b500ff3c0b1.arrow\n",
            "Grouping texts in chunks of 1024: 100% 1/1 [00:00<00:00, 168.24 examples/s]\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 13.2MB/s]\n",
            "model.safetensors:   6% 189M/3.13G [00:03<00:29, 102MB/s] [INFO|trainer.py:2303] 2024-10-18 18:04:38,168 >> ***** Running training *****\n",
            "[INFO|trainer.py:2304] 2024-10-18 18:04:38,168 >>   Num examples = 18\n",
            "[INFO|trainer.py:2305] 2024-10-18 18:04:38,168 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2306] 2024-10-18 18:04:38,168 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:2309] 2024-10-18 18:04:38,168 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2310] 2024-10-18 18:04:38,168 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2311] 2024-10-18 18:04:38,168 >>   Total optimization steps = 9\n",
            "[INFO|trainer.py:2312] 2024-10-18 18:04:38,170 >>   Number of trainable parameters = 774,030,080\n",
            "[INFO|integration_utils.py:812] 2024-10-18 18:04:38,176 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "model.safetensors:   7% 210M/3.13G [00:03<00:33, 87.1MB/s]\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "model.safetensors:   8% 252M/3.13G [00:03<00:31, 92.0MB/s]\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "model.safetensors:  54% 1.69G/3.13G [00:22<00:20, 69.1MB/s]"
          ]
        }
      ]
    }
  ]
}